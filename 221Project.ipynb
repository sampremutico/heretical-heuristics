{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://blog.dataiku.com/2016/10/08/machine-learning-markov-chains-generate-clinton-trump-quotes\n",
    "#TODO: get a lot more text for each author, \n",
    "#implement starting a phrase with a <sentence begin> and ending it with a <sentence end>\n",
    "#implement Backoff, \n",
    "#implement word weights (right now the ngram densities are too low for weights to do much) for overall and in-dialogue\n",
    "#graph search\n",
    "#A state is the N words at the end of the sentence, and the list of magic words. A transition between states is the adding of a new word to the end of res. \n",
    "#Ngrams alg defines neighbors\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "MAX_DIALOGUE_LEN = 35\n",
    "AUTHOR_AUG = \"Augistine\"\n",
    "AUTHOR_ZZ = \"Zhuangzi\"\n",
    "MAGIC_WORDS = set([])#[\"god\",\"man\",\"world\",\"death\",\"heaven\",\"life\",\"time\",\"body\",\"mind\"])\n",
    "grams1 = {}\n",
    "grams2 = {}\n",
    "grams3 = {}\n",
    "empty_words = ['.',',',':','i','the','should','it','for','by','this','they','them','because','so','is','and','a','with','to','\\'', '\\\"','be','not','no','thus','in','have','as','but','from','on','do','at','or','an','will','my','so','if', 'these', 'those','of','was','there','then','what','we','its','it\\'s', 'as','into','that','may','are','he','can','how','such','were','which','\\'','who','him','any','one','his','like','about','when','has','things','yet','all','said']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gramlib(object):\n",
    "    def __init__(self, gdict = [], start_list = []):\n",
    "        self.gdict = gdict\n",
    "        self.startList = start_list\n",
    "    def getPost(self,pre):\n",
    "        return self.gdict[pre]\n",
    "    def getStarter(self):\n",
    "        return random.choice(self.startList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_grams(grams, textfile): #initialize the dicts that store the markov probabilities for N-word chains\n",
    "    f = open(textfile)\n",
    "    s = f.read().replace('(','').replace(')','')\n",
    "    t = nltk.word_tokenize(s)\n",
    "    start_list = []\n",
    "    for i in range(len(t)-(N)):\n",
    "        trigram = tuple(t[x].lower() for x in range(i,i+N))\n",
    "        if(t[i-1]==\".\" or t[i-1]==\"!\" or t[i-1]==\"?\"):\n",
    "            if(t[i]!=\" \" and t[i]!=\",\" and t[i]!=\"\\'\" and t[i]!=\".\"):\n",
    "                start_list.append(trigram)\n",
    "        if trigram in grams.keys():\n",
    "            grams[trigram].add(t[i+N].lower())\n",
    "        else:\n",
    "            grams[trigram] = set([t[i+N].lower()])\n",
    "    g = gramlib(grams, start_list)\n",
    "    return g\n",
    "\n",
    "            \n",
    "def ngram_generate(grams): #generate text from a dict passed in\n",
    "    nwords = 0\n",
    "    start = grams.getStarter()\n",
    "    res = list(start)\n",
    "    while(nwords < 100):\n",
    "        pre = tuple(res[(-1*N):])\n",
    "        nextword = random.choice(list(grams[pre]))\n",
    "        res.append(nextword)\n",
    "        if nextword == '.' or nextword =='?': break\n",
    "        nwords +=1\n",
    "    if nwords ==100: res.append('-')\n",
    "    return \" \".join(res).replace(' .', '.').replace(' ,', ',').replace(' ;',';').replace(' ?', '?').replace(' \\'', '\\'').replace(' !', '!')\n",
    "\n",
    "def solve_density(grams): #On average, how many words are in the dict for each ngram?\n",
    "    total = 0.0\n",
    "    div = 0\n",
    "    for word in grams.keys():\n",
    "        if(len(grams[word])>1):\n",
    "            total += len(grams[word])\n",
    "            div +=1\n",
    "    return total/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize_grams(grams1, 'augustine_full.txt')\n",
    "#initialize_grams(grams1, 'zhuangzi.txt')\n",
    "#solve_density(grams1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramSearchProblem(object):\n",
    "    def __init__(self, N, grams, magic_words):\n",
    "        self.N = N\n",
    "        self.grams = grams\n",
    "        self.magic_words = magic_words\n",
    "        self.startActions = None\n",
    "\n",
    "    # Trivially return 100 if word is magic word, 1 otherwise\n",
    "    def ngram_cost(self,state):\n",
    "        if state[-1] in self.magic_words:\n",
    "            return 30#try different weights\n",
    "        if state[-1] == \".\" or state[-1] == \"?\" or state[-1] == \"!\":\n",
    "            return 5\n",
    "        return 1\n",
    "    \n",
    "    def ngram_generate_next_words(self, curr_gram):\n",
    "        pre = tuple(curr_gram[(-1*N):])\n",
    "        l =self.grams.getPost(pre)\n",
    "        pres = []\n",
    "        for i in range(len(curr_gram)-N):\n",
    "            pres.append(curr_gram[i:i+N])\n",
    "        suffix = curr_gram[-1*(N-1):]\n",
    "        result = [w for w in l if suffix+[w] not in pres]\n",
    "        final_result = [p for p in result if curr_gram.count(p)<2]\n",
    "        return final_result\n",
    "       # print(\"pres >>>>\", pres)\n",
    "        #print(\"res >>>>>\", result)\n",
    "        #if (len(result)!=len(l)):\n",
    "         #   print('success')\n",
    "        return result\n",
    "    \n",
    "    def startState(self):\n",
    "        start = self.grams.getStarter()\n",
    "        self.startActions = list(start)\n",
    "        return list(start)\n",
    "\n",
    "    def isEnd(self, state):\n",
    "        return len(state) >= MAX_DIALOGUE_LEN or state[-1][-1] == \".\" or state[-1][-1] == \"?\" or state[-1][-1] == \"!\"#max blurb length or last word ends in period\n",
    "    \n",
    "    def succAndCost(self, state):\n",
    "        result = []\n",
    "        possible_next_words = self.ngram_generate_next_words(state)\n",
    "        for next_word in possible_next_words:\n",
    "            next_state = state[:]+[next_word]\n",
    "            result.append((next_word, next_state, self.ngram_cost(next_state)))\n",
    "        return result\n",
    "    def getMagic(self):\n",
    "        return self.magic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backtrackingSearch(problem):\n",
    "    bestTotalCost = [float('-inf')]\n",
    "    bestHistory = [None]\n",
    "    def recurse(state,curr_history,curr_cost):\n",
    "        if problem.isEnd(state):\n",
    "            if curr_cost > bestTotalCost[0]:\n",
    "                bestTotalCost[0], bestHistory[0] = curr_cost, curr_history \n",
    "            return\n",
    "        for action, next_state, cost in problem.succAndCost(state):\n",
    "            recurse(next_state, curr_history+[action], curr_cost+cost)\n",
    "    \n",
    "    recurse(problem.startState(),problem.startState(),0)\n",
    "    bestHistory = procStr(\" \".join(bestHistory[0]))\n",
    "   # print(\"<-----------Best History--------->\")\n",
    "    #print(bestHistory)return \n",
    "    #print(\"<--------------Score------------->\")\n",
    "    #print(bestTotalCost[0])\n",
    "    return bestHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kBeamsSearch(problem,k = 2500):\n",
    "    C = []\n",
    "    for i in range(k):\n",
    "        s = problem.startState()\n",
    "        i = 0\n",
    "        for w in problem.getMagic():\n",
    "            if w in s:\n",
    "                i += 30\n",
    "        C.append((s,s,i))\n",
    "    complete_beams = []\n",
    "    while len(complete_beams) < k:\n",
    "        for j in range(len(C)):\n",
    "            curr_beam = C[j][:]\n",
    "            for action, next_state, cost in problem.succAndCost(curr_beam[0]):    \n",
    "                C.append((next_state, curr_beam[1]+[action],curr_beam[2]+cost))\n",
    "        C = sorted(C, key=lambda x: x[2])[::-1]\n",
    "        C = C if len(C) < k else C[:k]\n",
    "        for beam in C:\n",
    "            if problem.isEnd(beam[0]):\n",
    "                complete_beams.append(beam)  \n",
    "        new_beams = [b for b in C if b not in complete_beams]\n",
    "        C = new_beams\n",
    "    res = complete_beams[-1]\n",
    "    return procStr(\" \".join(res[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#blurb = backtrackingSearch(NGramSearchProblem(N,grams1,MAGIC_WORDS)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But how seldom it is that he who does so is sure to be the last in going backwards.'"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigs = []\n",
    "text1 = 'zhuangzi.txt'\n",
    "g1 = initialize_grams(grams1, text1)\n",
    "speech1 = kBeamsSearch(NGramSearchProblem(N,g1,set(list(MAGIC_WORDS)+sigs)))\n",
    "speech1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo - get rid of bad punctuation\n",
    "#capitalize first letter\n",
    "#end with a hyphen if they are cut off.\n",
    "def convo(text1, text2, author1, author2):\n",
    "    grams1 = {}\n",
    "    grams2 = {}\n",
    "    g1 = initialize_grams(grams1, text1)\n",
    "    g2 = initialize_grams(grams2, text2)\n",
    "    print('first ngram density: '+ str(solve_density(grams1))+ '\\n')\n",
    "    print('second ngram density: '+ str(solve_density(grams2))+ '\\n')\n",
    "    sigs = []\n",
    "    for i in range(5):\n",
    "        print(author1 + ':')\n",
    "        #speech1 = ngram_generate(grams1)\n",
    "        #speech1 = backtrackingSearch(NGramSearchProblem(N,g1,set(list(MAGIC_WORDS)+sigs)))\n",
    "        speech1 = kBeamsSearch(NGramSearchProblem(N,g1,set(list(MAGIC_WORDS)+sigs)))\n",
    "        sigs = getSigWords(speech1)\n",
    "        print(speech1)\n",
    "        print('\\n')\n",
    "        #print(sigs)\n",
    "        print('\\n')\n",
    "        #speech2 = ngram_generate(grams2)\n",
    "        #speech2 = backtrackingSearch(NGramSearchProblem(N,g2,set(list(MAGIC_WORDS)+sigs)))\n",
    "        speech2 = kBeamsSearch(NGramSearchProblem(N,g2,set(list(MAGIC_WORDS)+sigs)))\n",
    "        sigs = getSigWords(speech2)\n",
    "        print(author2 + ':')\n",
    "        print(speech2)\n",
    "        print('\\n')\n",
    "        #print(sigs)\n",
    "        print('\\n')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-615-676acf3e6d9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zhuangzi.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'augustine_full.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Zhuangzi'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Augustine'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-614-23ca19a7b4bc>\u001b[0m in \u001b[0;36mconvo\u001b[0;34m(text1, text2, author1, author2)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgrams2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrams1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrams2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first ngram density: '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolve_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrams1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'second ngram density: '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolve_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrams2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-562-b93f250fa202>\u001b[0m in \u001b[0;36minitialize_grams\u001b[0;34m(grams, textfile)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mstart_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sjkenned/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    132\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/home/sjkenned/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 132\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m/home/sjkenned/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "convo('zhuangzi.txt','augustine_full.txt', 'Zhuangzi','Augustine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSigWords(str):\n",
    "    s = set(nltk.word_tokenize(str))\n",
    "    for w in empty_words:\n",
    "        if w in s: s.remove(w)\n",
    "    return list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def procStr(str):\n",
    "    newstr = str.replace('0','').replace('1','').replace('2','').replace('3','').replace('4','').replace('5','').replace('6','').replace('7','').replace('8','').replace('9','').replace('`','').replace('\\'','').replace(' .', '.').replace(' ,', ',').replace(' ;',';').replace(' ?', '?').replace(' \\'', '\\'').replace(' !', '!').replace(' i ', ' I ').replace(' i ', ' I ').replace(' ;',';')\n",
    "    if (newstr[-1]!='.' and newstr[-1]!='?' and newstr[-1]!='!'):\n",
    "        newstr = newstr[:] + '-'\n",
    "    return newstr[0].upper() + newstr[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triple_conv(text1,text2,text3,author1,author2,author3):\n",
    "    grams1 = {}\n",
    "    grams2 = {}\n",
    "    grams3 = {}\n",
    "    g1 = initialize_grams(grams1, text1)\n",
    "    g2 = initialize_grams(grams2, text2)\n",
    "    g3 = initialize_grams(grams3, text3)\n",
    "    print('first ngram density_v2: '+ str(solve_density(grams1))+ '\\n')\n",
    "    print('second ngram density_v2: '+ str(solve_density(grams2))+ '\\n')\n",
    "    print('third ngram density_v2: '+ str(solve_density(grams3))+ '\\n')\n",
    "    sigs = []\n",
    "    for i in range(3):\n",
    "        print(author1 + ':')\n",
    "        #speech1 = ngram_generate(grams1)\n",
    "        speech1 = kBeamsSearch(NGramSearchProblem(N,g1,set(list(MAGIC_WORDS)+sigs)))\n",
    "        sigs = getSigWords(speech1)\n",
    "        print(speech1)\n",
    "        print('\\n')\n",
    "        #print(sigs)\n",
    "        print('\\n')\n",
    "        #speech2 = ngram_generate(grams2)\n",
    "        speech2 = kBeamsSearch(NGramSearchProblem(N,g2,set(list(MAGIC_WORDS)+sigs)))\n",
    "        sigs = getSigWords(speech2)\n",
    "        print(author2 + ':')\n",
    "        print(speech2)\n",
    "        print('\\n')\n",
    "        #print(sigs)\n",
    "        print('\\n')\n",
    "        speech3 = kBeamsSearch(NGramSearchProblem(N,g3,set(list(MAGIC_WORDS)+sigs)))\n",
    "        sigs = getSigWords(speech3)\n",
    "        print(author3 + ':')\n",
    "        print(speech3)\n",
    "        print('\\n')\n",
    "        #print(sigs)\n",
    "        print('\\n')\n",
    "        i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ngram density_v2: 2.954440026507621\n",
      "\n",
      "second ngram density_v2: 3.4217966453397892\n",
      "\n",
      "third ngram density_v2: 3.7854438458709243\n",
      "\n",
      "Zhuangzi:\n",
      "By my constant practice of it, and it is so with all things, and the other wrong?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Augustine:\n",
      "And the other with the other, but that it might be understood to be gods, but of those who had discerned them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Plato:\n",
      "That of the gods; and the other gods, and of other things, which are practically indestructible.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Zhuangzi:\n",
      "Open no other door; employ no other medicine; dwell with him as with a friend in the same circumstances to one another.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Augustine:\n",
      "At the same time; and it is true; another way, and the other philosophers for no other end, but that I know not.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Plato:\n",
      "Very true; and the same time in the same way of other practices?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Zhuangzi:\n",
      "In the same way; and in this other men imitate him; how much more will other men do, they would have no wrangling.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Augustine:\n",
      "How much more appropriately would that god who is not the way, and the other with men; if they are good; and this too would have been endowed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Plato:\n",
      "Good; and that no other way; and you are the men, too, hermogenes.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triple_conv('zhuangzi.txt','augustine_full.txt','plato_full.txt','Zhuangzi', 'Augustine', 'Plato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = ['s','a','b','t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = ['s','a','b','t','n','z','t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-87eead358601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "z.remove([a for a in g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.append('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = ['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
